{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3 - Modeling\n",
    "\n",
    "**Note 1: the following starting code only generates a single random train/test split when default_seed is used. You need to modify the code to generate 100 independent train/test splits with different seeds and report the average results on those independent splits along with standard deviation.**\n",
    "\n",
    "**Note 2: You are completely free to use your own implementation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the Jupyter Notebook that contains all code necessary to run the Regressors and Sensitivity of Portfolio Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load general utilities\n",
    "# ----------------------\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.axes as ax\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import seaborn as sns\n",
    "\n",
    "# Load sklearn utilities\n",
    "# ----------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, roc_curve, brier_score_loss, mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Load classifiers\n",
    "# ----------------\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Other Packages\n",
    "# --------------\n",
    "from scipy.stats import kendalltau\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "from scipy.interpolate import spline\n",
    "\n",
    "# Load debugger, if required\n",
    "#import pixiedust\n",
    "pd.options.mode.chained_assignment = None #'warn'\n",
    "\n",
    "# suppress all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that, given a CVGridSearch object, finds the\n",
    "# percentage difference between the best and worst scores\n",
    "\n",
    "def find_score_variation(cv_model):\n",
    "    all_scores = cv_model.cv_results_['mean_test_score']\n",
    "    return( np.abs((max(all_scores) - min(all_scores))) * 100 / max(all_scores) )\n",
    "\n",
    "    '''\n",
    "    which_min_score = np.argmin(all_scores)\n",
    "    \n",
    "    all_perc_diff = []\n",
    "    \n",
    "    try:\n",
    "        all_perc_diff.append( np.abs(all_scores[which_min_score - 1] - all_scores[which_min_score])*100 / min(all_scores) )\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        all_perc_diff.append( np.abs(all_scores[which_min_score + 1] - all_scores[which_min_score])*100 / min(all_scores) )\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return ( np.mean(all_perc_diff) )\n",
    "    '''\n",
    "\n",
    "# Define a function that checks, given a CVGridSearch object,\n",
    "# whether the optimal parameters lie on the edge of the search\n",
    "# grid\n",
    "def find_opt_params_on_edge(cv_model):\n",
    "    out = False\n",
    "    \n",
    "    for i in cv_model.param_grid:\n",
    "        if cv_model.best_params_[i] in [ cv_model.param_grid[i][0], cv_model.param_grid[i][-1] ]:\n",
    "            out = True\n",
    "            break\n",
    "            \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a default random seed and an output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_seed = 1\n",
    "output_file = \"output_sample\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to print a line to our output file\n",
    "\n",
    "def dump_to_output(key, value):\n",
    "    with open(output_file, \"a\") as f:\n",
    "        f.write(\",\".join([str(default_seed), key, str(value)]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data and engineer the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data and features from the pickle file saved in CS-Phase 2\n",
    "data, discrete_features, continuous_features, ret_cols = pickle.load( open( \"dataclean_data.pickle\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the outcome columns: True if loan_status is either Charged Off or Default, False otherwise\n",
    "\n",
    "loan_stats= data['loan_status']\n",
    "outcome_values = []\n",
    "for value in loan_stats:\n",
    "    if value == 'Charged Off' or value == 'Default':\n",
    "        outcome_values.append(True)\n",
    "    else:\n",
    "        outcome_values.append(False)\n",
    "data[\"outcome\"] = outcome_values\n",
    "\n",
    "mask = np.array(loan_stats == 'Charged Off')\n",
    "mask_2 = np.array(loan_stats == 'Default')\n",
    "\n",
    "outcome_array = np.array(outcome_values)\n",
    "mask = np.array(outcome_array == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature for the length of a person's credit history at the time the loan is issued\n",
    "data['cr_hist'] = (data.issue_d - data.earliest_cr_line) / np.timedelta64(1, 'M')\n",
    "continuous_features.append('cr_hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly assign each row to a training and test set. \n",
    "#We do this now because we will be fitting a variety of models on various time periods,\n",
    "#and we would like every period to use the *same* training/test split\n",
    "np.random.seed(default_seed)\n",
    "## create the train columns where the value is True if it is a train instance\n",
    "#and False otherwise. Hint: use np.random.choice with 70% for training and 30% for testing\n",
    "\n",
    "train_array = np.array([True, False])\n",
    "train = np.random.choice(train_array, data.shape[0], p=[0.7, 0.3])\n",
    "data['train'] = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix of features and outcomes, with dummies. Record the names of the dummies for later use\n",
    "X_continuous = data[continuous_features].values\n",
    "\n",
    "X_discrete = pd.get_dummies(data[discrete_features], dummy_na = True, prefix_sep = \"::\", drop_first = True)\n",
    "discrete_features_dummies = X_discrete.columns.tolist()\n",
    "X_discrete = X_discrete.values\n",
    "\n",
    "X = np.concatenate( (X_continuous, X_discrete), axis = 1 )\n",
    "\n",
    "y = data.outcome.values\n",
    "\n",
    "train = data.train.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare functions to fit and evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data_subset = np.array([True]*len(data)),\n",
    "                    n_samples_train = 30000,\n",
    "                    n_samples_test = 20000,\n",
    "                    feature_subset = None,\n",
    "                    date_range_train = (data.issue_d.min(), data.issue_d.max()),\n",
    "                    date_range_test = (data.issue_d.min(), data.issue_d.max()),\n",
    "                    random_state = default_seed):\n",
    "    '''\n",
    "    This function will prepare the data for classification or regression.\n",
    "    It expects the following parameters:\n",
    "      - data_subset: a numpy array with as many entries as rows in the\n",
    "                     dataset. Each entry should be True if that row\n",
    "                     should be used, or False if it should be ignored\n",
    "      - n_samples_train: the total number of samples to be used for training.\n",
    "                         Will trigger an error if this number is larger than\n",
    "                         the number of rows available after all filters have\n",
    "                         been applied\n",
    "      - n_samples_test: as above for testing\n",
    "      - feature_subect: A list containing the names of the features to be\n",
    "                        used in the model. In None, all features in X are\n",
    "                        used\n",
    "      - date_range_train: a tuple containing two dates. All rows with loans\n",
    "                          issued outside of these two dates will be ignored in\n",
    "                          training\n",
    "      - date_range_test: as above for testing\n",
    "      - random_state: the random seed to use when selecting a subset of rows\n",
    "      \n",
    "    Note that this function assumes the data has a \"Train\" column, and will\n",
    "    select all training rows from the rows with \"True\" in that column, and all\n",
    "    the testing rows from those with a \"False\" in that column.\n",
    "    \n",
    "    This function returns a dictionary with the following entries\n",
    "      - X_train: the matrix of training data\n",
    "      - y_train: the array of training labels\n",
    "      - train_set: a Boolean vector with as many entries as rows in the data\n",
    "                  that denotes the rows that were used in the train set\n",
    "      - X_test: the matrix of testing data\n",
    "      - y_test: the array of testing labels\n",
    "      - test_set: a Boolean vector with as many entries as rows in the data\n",
    "                  that denotes the rows that were used in the test set\n",
    "    '''\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "        \n",
    "    # Filter down the data to the required date range, and downsample\n",
    "    # as required\n",
    "    filter_train = ( train & (data.issue_d >= date_range_train[0]) &\n",
    "                            (data.issue_d <= date_range_train[1]) & data_subset ).values\n",
    "    #print('FILTER TRAIN BEFORE', filter_train, len(filter_train))\n",
    "    filter_test = ( (train == False) & (data.issue_d >= date_range_test[0])\n",
    "                            & (data.issue_d <= date_range_test[1]) & data_subset ).values\n",
    "    \n",
    "    filter_train[ np.random.choice( np.where(filter_train)[0], size = filter_train.sum()\n",
    "                                                   - n_samples_train, replace = False ) ] = False\n",
    "    filter_test[ np.random.choice( np.where(filter_test)[0], size = filter_test.sum()\n",
    "                                                  - n_samples_test, replace = False ) ] = False\n",
    "    #print('AFTER: ', filter_train)\n",
    "    #print(len(filter_train))\n",
    "    # Prepare the training and test set\n",
    "    X_train = X[ filter_train , :]\n",
    "    X_test = X[ filter_test, :]\n",
    "    #print('after filtering: ', len(X_train), len(X_test))\n",
    "    if feature_subset != None:\n",
    "        cols = [i for i, j in enumerate(continuous_features + discrete_features_dummies)\n",
    "                                                     if j.split(\"::\")[0] in feature_subset]\n",
    "        X_train = X_train[ : , cols ]\n",
    "        X_test = X_test[ : , cols ]\n",
    "        \n",
    "    y_train = y[ filter_train ]\n",
    "    y_test = y[ filter_test ]\n",
    "    \n",
    "    # Scale the variables\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    #print(len(X_train), len(X_test))\n",
    "    # return training and testing data\n",
    "    out = {'X_train':X_train, 'y_train':y_train, 'train_set':filter_train, \n",
    "           'X_test':X_test, 'y_test':y_test, 'test_set':filter_test}\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_classification(model, data_dict,\n",
    "                          cv_parameters = {},\n",
    "                          model_name = None,\n",
    "                          random_state = default_seed,\n",
    "                          output_to_file = True,\n",
    "                          print_to_screen = True):\n",
    "    '''\n",
    "    This function will fit a classification model to data and print various evaluation\n",
    "    measures. It expects the following parameters\n",
    "      - model: an sklearn model object\n",
    "      - data_dict: the dictionary containing both training and testing data;\n",
    "                   returned by the prepare_data function\n",
    "      - cv_parameters: a dictionary of parameters that should be optimized\n",
    "                       over using cross-validation. Specifically, each named\n",
    "                       entry in the dictionary should correspond to a parameter,\n",
    "                       and each element should be a list containing the values\n",
    "                       to optimize over\n",
    "      - model_name: the name of the model being fit, for printouts\n",
    "      - random_state: the random seed to use\n",
    "      - output_to_file: if the results will be saved to the output file\n",
    "      - print_to_screen: if the results will be printed on screen\n",
    "    \n",
    "    If the model provided does not have a predict_proba function, we will\n",
    "    simply print accuracy diagnostics and return.\n",
    "    \n",
    "    If the model provided does have a predict_proba function, we first\n",
    "    figure out the optimal threshold that maximizes the accuracy and\n",
    "    print out accuracy diagnostics. We then print an ROC curve, sensitivity/\n",
    "    specificity curve, and calibration curve.\n",
    "    \n",
    "    This function returns a dictionary with the following entries\n",
    "      - model: the best fitted model\n",
    "      - y_pred: predictions for the test set\n",
    "      - y_pred_probs: probability predictions for the test set, if the model\n",
    "                      supports them\n",
    "      - y_pred_score: prediction scores for the test set, if the model does not \n",
    "                      output probabilities.\n",
    "    '''\n",
    "        \n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # --------------------------\n",
    "    #   Step 1 - Load the data\n",
    "    # --------------------------\n",
    "    X_train = data_dict['X_train']\n",
    "    y_train = data_dict['y_train']\n",
    "    \n",
    "    #print('before SMOTE: ', len(X_train), len(y_train))\n",
    "    X_test = data_dict['X_test']\n",
    "    y_test = data_dict['y_test']\n",
    "    \n",
    "    filter_train = data_dict['train_set']    \n",
    "\n",
    "    # --------------------------\n",
    "    #   Step 2 - Fit the model\n",
    "    # --------------------------\n",
    "    \n",
    "    cv_model = GridSearchCV(model, cv_parameters)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    cv_model.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    best_model = cv_model.best_estimator_\n",
    "    \n",
    "    if print_to_screen:\n",
    "\n",
    "        if model_name != None:\n",
    "            print(\"=========================================================\")\n",
    "            print(\"  Model: \" + model_name)\n",
    "            print(\"=========================================================\")\n",
    "\n",
    "        print(\"Fit time: \" + str(round(end_time - start_time, 2)) + \" seconds\")\n",
    "        print(\"Optimal parameters:\")\n",
    "        print(cv_model.best_params_)\n",
    "        print(\"\")\n",
    "    \n",
    "    # -------------------------------\n",
    "    #   Step 3 - Evaluate the model\n",
    "    # -------------------------------\n",
    "    \n",
    "    # If possible, make probability predictions\n",
    "    try:\n",
    "        y_pred_probs = best_model.predict_proba(X_test)[:,1]\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)\n",
    "        \n",
    "        probs_predicted = True\n",
    "    except:\n",
    "        probs_predicted = False\n",
    "    \n",
    "   # print('is this probs_predicted (can it use predict_proba)', probs_predicted)\n",
    "    # Make predictions; if we were able to find probabilities, use\n",
    "    # the threshold that maximizes the accuracy in the training set.\n",
    "    # If not, just use the learner's predict function\n",
    "    if probs_predicted:\n",
    "       # print(len(X_train))\n",
    "        y_train_pred_probs = best_model.predict_proba(X_train)[:,1]\n",
    "        fpr_train, tpr_train, thresholds_train = roc_curve(y_train, y_train_pred_probs)\n",
    "        #print('Y_TRAIN PRED PROBS', len(y_train_pred_probs))\n",
    "        true_pos_train = tpr_train*(y_train.sum())\n",
    "        true_neg_train = (1 - fpr_train) *(1-y_train).sum()\n",
    "        \n",
    "        #Add\n",
    "        best_threshold = 0.5\n",
    "        \n",
    "        if print_to_screen:\n",
    "            print(\"Chosen threshold was: \" + str(best_threshold))\n",
    "        \n",
    "        y_pred = (y_pred_probs > best_threshold)\n",
    "    else:\n",
    "        y_pred = best_model.predict(X_test)\n",
    "    #We created these to compute averages later.\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    other_scores = classification_report(y_test, y_pred, target_names =['No default', 'Default'], digits = 4, output_dict=True)\n",
    "    if print_to_screen:\n",
    "        print(\"Accuracy: \", accuracy)\n",
    "        print(classification_report(y_test, y_pred, target_names =['No default', 'Default'], digits = 4))\n",
    "\n",
    "    if print_to_screen:\n",
    "        if probs_predicted:        \n",
    "            plt.figure(figsize = (13, 4.5))\n",
    "            plt.subplot(2, 2, 1)\n",
    "\n",
    "            plt.title(\"ROC Curve (AUC = %0.2f)\"% roc_auc_score(y_test, y_pred_probs))\n",
    "            plt.plot(fpr, tpr, 'b')\n",
    "            plt.plot([0,1],[0,1],'r--')\n",
    "            plt.xlim([0,1]); plt.ylim([0,1])\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.xlabel('False Positive Rate')\n",
    "\n",
    "            plt.subplot(2, 2, 3)\n",
    "\n",
    "            plt.plot(thresholds, tpr, 'b', label = 'Sensitivity')\n",
    "            plt.plot(thresholds, 1 -fpr, 'r', label = 'Specificity')\n",
    "            plt.legend(loc = 'lower right')\n",
    "            plt.xlim([0,1]); plt.ylim([0,1])\n",
    "            plt.xlabel('Threshold')\n",
    "\n",
    "            plt.subplot(2, 2, 2)\n",
    "\n",
    "            fp_0, mpv_0 = calibration_curve(y_test, y_pred_probs, n_bins = 10)\n",
    "            plt.plot([0,1], [0,1], 'k:', label='Perfectly calibrated')\n",
    "            plt.plot(mpv_0, fp_0, 's-')\n",
    "            plt.ylabel('Fraction of Positives')\n",
    "            plt.xlim([0,1]); plt.ylim([0,1])\n",
    "            plt.legend(loc ='upper left')\n",
    "            \n",
    "            plt.subplot(2, 2, 4)\n",
    "            plt.hist(y_pred_probs, range=(0, 1), bins=10, histtype=\"step\", lw=2)\n",
    "            plt.xlim([0,1]); plt.ylim([0,20000])\n",
    "            plt.xlabel('Mean Predicted Probability')\n",
    "            plt.ylabel('Count')\n",
    "            \n",
    "            #plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "    # Additional Score Check\n",
    "    if probs_predicted:\n",
    "        y_train_score = y_train_pred_probs\n",
    "    else:\n",
    "        y_train_score = best_model.decision_function(X_train)\n",
    "   \n",
    "   # print(len(y_train_score), len(data.grade[filter_train]))\n",
    "    tau, p_value = kendalltau(y_train_score, data.grade[filter_train])\n",
    "    if print_to_screen:\n",
    "        print(\"\")\n",
    "        print(\"Similarity to LC grade ranking: \", tau)\n",
    "    \n",
    "    if probs_predicted:\n",
    "        brier_score = brier_score_loss(y_test, y_pred_probs)\n",
    "        if print_to_screen:\n",
    "            print(\"Brier score:\", brier_score)\n",
    "    \n",
    "    # Return the model predictions, and the\n",
    "    # test set\n",
    "    # -------------------------------------\n",
    "    out = {'model':best_model, 'y_pred_labels':y_pred}\n",
    "    \n",
    "    if probs_predicted:\n",
    "        out.update({'y_pred_probs':y_pred_probs})\n",
    "    else:\n",
    "        y_pred_score = best_model.decision_function(X_test)\n",
    "        out.update({'y_pred_score':y_pred_score})\n",
    "        \n",
    "    # Output results to file\n",
    "    # ----------------------\n",
    "    if probs_predicted and output_to_file:\n",
    "        # Check whether any of the CV parameters are on the edge of\n",
    "        # the search space\n",
    "        opt_params_on_edge = find_opt_params_on_edge(cv_model)\n",
    "        dump_to_output(model_name + \"::search_on_edge\", opt_params_on_edge)\n",
    "        if print_to_screen:\n",
    "            print(\"Were parameters on edge? : \" + str(opt_params_on_edge))\n",
    "        \n",
    "        # Find out how different the scores are for the different values\n",
    "        # tested for by cross-validation. If they're not too different, then\n",
    "        # even if the parameters are off the edge of the search grid, we should\n",
    "        # be ok\n",
    "        score_variation = find_score_variation(cv_model)\n",
    "        dump_to_output(model_name + \"::score_variation\", score_variation)\n",
    "        if print_to_screen:\n",
    "            print(\"Score variations around CV search grid : \" + str(score_variation))\n",
    "        \n",
    "        # Print out all the scores\n",
    "        dump_to_output(model_name + \"::all_cv_scores\", str(cv_model.cv_results_['mean_test_score']))\n",
    "        if print_to_screen:\n",
    "            print( str(cv_model.cv_results_['mean_test_score']) )\n",
    "        \n",
    "        # Dump the AUC to file\n",
    "        dump_to_output(model_name + \"::roc_auc\", roc_auc_score(y_test, y_pred_probs) )\n",
    "        \n",
    "    return out, accuracy, other_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test different machine learning classification models\n",
    "\n",
    "The machine learning models listed in the following are just our suggestions. You are free to try any other models that you would like to experiment with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define your set of features to use in different models\n",
    "continuous_features.remove('funded_amnt')\n",
    "discrete_features_dummies=[val for val in discrete_features_dummies \n",
    "                           if val not in ['purpose::nan','verification_status::nan','grade::nan', 'term::nan', 'home_ownership::nan', 'emp_length::nan']]\n",
    "your_features = discrete_features_dummies+continuous_features\n",
    "\n",
    "# regression_features=data_dummies.columns+continuous_features+ret_cols\n",
    "# prepare the train, test data for training models\n",
    "data_dictionaries = []\n",
    "for i in range(100):\n",
    "    data_dict = prepare_data(feature_subset = your_features, random_state=i)\n",
    "    data_dictionaries.append(data_dict)\n",
    "    \n",
    "all_features = pd.Series(continuous_features + discrete_features_dummies)\n",
    "idx = [i for i, j in enumerate(continuous_features + discrete_features_dummies)\n",
    "       if j.split(\"::\")[0] in your_features]\n",
    "selected_features = all_features[idx]\n",
    "selected_features.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_dic=prepare_data(n_samples_train=644022, n_samples_test=276032, feature_subset=your_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove features and prepare data\n",
    "your_features_without_derived=[val for val in your_features if val not in ['grade','installment','int_rate']]\n",
    "entire_dic_without_derived=prepare_data(n_samples_train=644022, n_samples_test=276032, feature_subset=your_features_without_derived)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We added this\n",
    "def prepare_data_reg(data_subset = np.array([True]*len(data)),\n",
    "                    n_samples_train = 30000,\n",
    "                    n_samples_test = 20000,\n",
    "                    feature_subset = None,\n",
    "                    date_range_train = (data.issue_d.min(), data.issue_d.max()),\n",
    "                    date_range_test = (data.issue_d.min(), data.issue_d.max()),\n",
    "                    random_state = default_seed):\n",
    "    '''\n",
    "    This function will prepare the data for classification or regression.\n",
    "    It expects the following parameters:\n",
    "      - data_subset: a numpy array with as many entries as rows in the\n",
    "                     dataset. Each entry should be True if that row\n",
    "                     should be used, or False if it should be ignored\n",
    "      - n_samples_train: the total number of samples to be used for training.\n",
    "                         Will trigger an error if this number is larger than\n",
    "                         the number of rows available after all filters have\n",
    "                         been applied\n",
    "      - n_samples_test: as above for testing\n",
    "      - feature_subect: A list containing the names of the features to be\n",
    "                        used in the model. In None, all features in X are\n",
    "                        used\n",
    "      - date_range_train: a tuple containing two dates. All rows with loans\n",
    "                          issued outside of these two dates will be ignored in\n",
    "                          training\n",
    "      - date_range_test: as above for testing\n",
    "      - random_state: the random seed to use when selecting a subset of rows\n",
    "      \n",
    "    Note that this function assumes the data has a \"Train\" column, and will\n",
    "    select all training rows from the rows with \"True\" in that column, and all\n",
    "    the testing rows from those with a \"False\" in that column.\n",
    "    \n",
    "    This function returns a dictionary with the following entries\n",
    "      - X_train: the matrix of training data\n",
    "      - y_train: the array of training labels\n",
    "      - train_set: a Boolean vector with as many entries as rows in the data\n",
    "                  that denotes the rows that were used in the train set\n",
    "      - X_test: the matrix of testing data\n",
    "      - y_test: the array of testing labels\n",
    "      - test_set: a Boolean vector with as many entries as rows in the data\n",
    "                  that denotes the rows that were used in the test set\n",
    "    '''\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "        \n",
    "    # Filter down the data to the required date range, and downsample\n",
    "    # as required\n",
    "    filter_train = ( train & (data.issue_d >= date_range_train[0]) &\n",
    "                            (data.issue_d <= date_range_train[1]) & data_subset ).values\n",
    "    #print('FILTER TRAIN BEFORE', filter_train, len(filter_train))\n",
    "    filter_test = ( (train == False) & (data.issue_d >= date_range_test[0])\n",
    "                            & (data.issue_d <= date_range_test[1]) & data_subset ).values\n",
    "    \n",
    "    filter_train[ np.random.choice( np.where(filter_train)[0], size = filter_train.sum()\n",
    "                                                   - n_samples_train, replace = False ) ] = False\n",
    "    filter_test[ np.random.choice( np.where(filter_test)[0], size = filter_test.sum()\n",
    "                                                  - n_samples_test, replace = False ) ] = False\n",
    "    #print('AFTER: ', filter_train)\n",
    "    #print(len(filter_train))\n",
    "    # Prepare the training and test set\n",
    "    X_train = X[ filter_train , :]\n",
    "    X_test = X[ filter_test, :]\n",
    "    #print('after filtering: ', len(X_train), len(X_test))\n",
    "    if feature_subset != None:\n",
    "        cols = [i for i, j in enumerate(continuous_features + discrete_features_dummies)\n",
    "                                                     if j.split(\"::\")[0] in feature_subset]\n",
    "        X_train = X_train[ : , cols ]\n",
    "        X_test = X_test[ : , cols ]\n",
    "        \n",
    "    y_train = y[ filter_train ]\n",
    "    y_test = y[ filter_test ]\n",
    "    \n",
    "    # Scale the variables\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # return training and testing data\n",
    "    out = {'X_train':X_train, 'y_train':y_train, 'train_set':filter_train, \n",
    "           'X_test':X_test, 'y_test':y_test, 'test_set':filter_test}\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_regression(model, data_dict,\n",
    "                      cv_parameters = {},\n",
    "                      separate = False, \n",
    "                      model_name = None,\n",
    "                      random_state = default_seed,\n",
    "                      output_to_file = True,\n",
    "                      print_to_screen = True):\n",
    "    '''\n",
    "    This function will fit a regression model to data and print various evaluation\n",
    "    measures. It expects the following parameters\n",
    "      - model: an sklearn model object\n",
    "      - data_dict: the dictionary containing both training and testing data;\n",
    "                   returned by the prepare_data function\n",
    "      - separate: a Boolean variable indicating whether we fit models for \n",
    "                  defaulted and non-defaulted loans separately\n",
    "      - cv_parameters: a dictionary of parameters that should be optimized\n",
    "                       over using cross-validation. Specifically, each named\n",
    "                       entry in the dictionary should correspond to a parameter,\n",
    "                       and each element should be a list containing the values\n",
    "                       to optimize over      \n",
    "      - model_name: the name of the model being fit, for printouts\n",
    "      - random_state: the random seed to use\n",
    "      - output_to_file: if the results will be saved to the output file\n",
    "      - print_to_screen: if the results will be printed on screen\n",
    "    \n",
    "    This function returns a dictionary FOR EACH RETURN DEFINITION with the following entries\n",
    "      - model: the best fitted model\n",
    "      - predicted_return: prediction result based on the test set\n",
    "      - predicted_regular_return: prediction result for non-defaulted loans (valid if separate == True)\n",
    "      - predicted_default_return: prediction result for defaulted loans (valid if separate == True)\n",
    "      - r2_scores: the testing r2_score(s) for the best fitted model\n",
    "    '''\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # --------------------------\n",
    "    #   Step 1 - Load the data\n",
    "    # --------------------------\n",
    "    \n",
    "    col_list = ['ret_PESS', 'ret_OPT', 'ret_INTa', 'ret_INTb']\n",
    "    \n",
    "    X_train = data_dict['X_train']\n",
    "    filter_train = data_dict['train_set']  \n",
    "\n",
    "    X_test = data_dict['X_test']\n",
    "    filter_test = data_dict['test_set']\n",
    "    out = {}\n",
    "#     print('before:',len(X_train))\n",
    "#     print('before:', len(y_train))\n",
    "#     print(data)\n",
    "    for ret_col in col_list:\n",
    "        \n",
    "        y_train = data.loc[filter_train, ret_col].as_matrix()\n",
    "        y_test = data.loc[filter_test, ret_col].as_matrix() \n",
    "\n",
    "        # --------------------------\n",
    "        #   Step 2 - Fit the model\n",
    "        # --------------------------\n",
    "\n",
    "        if separate:\n",
    "            outcome_train = data.loc[filter_train, 'outcome']\n",
    "            outcome_test = data.loc[filter_test, 'outcome']\n",
    "\n",
    "            # Train two separate regressors for defaulted and non-defaulted loans\n",
    "            X_train_0 = X_train[outcome_train == False]\n",
    "            y_train_0 = y_train[outcome_train == False]\n",
    "            X_test_0 = X_test[outcome_test == False]\n",
    "            y_test_0 = y_test[outcome_test == False]\n",
    "\n",
    "            X_train_1 = X_train[outcome_train == True]\n",
    "            y_train_1 = y_train[outcome_train == True]\n",
    "            X_test_1 = X_test[outcome_test == True]\n",
    "            y_test_1 = y_test[outcome_test == True]\n",
    "\n",
    "            cv_model_0 = GridSearchCV(model, cv_parameters, scoring='r2')\n",
    "            cv_model_1 = GridSearchCV(model, cv_parameters, scoring='r2')\n",
    "\n",
    "            start_time = time.time()\n",
    "            cv_model_0.fit(X_train_0, y_train_0)\n",
    "            cv_model_1.fit(X_train_1, y_train_1)\n",
    "            end_time = time.time()\n",
    "\n",
    "            best_model_0 = cv_model_0.best_estimator_\n",
    "            best_model_1 = cv_model_1.best_estimator_\n",
    "            \n",
    "            if print_to_screen:\n",
    "\n",
    "                if model_name != None:\n",
    "                    print(\"=========================================================\")\n",
    "                    print(\"  Model: \" + model_name + \"  Return column: \" + ret_col)\n",
    "                    print(\"=========================================================\")\n",
    "\n",
    "                print(\"Fit time: \" + str(round(end_time - start_time, 2)) + \" seconds\")\n",
    "                print(\"Optimal parameters:\")\n",
    "                print(\"model_0:\",cv_model_0.best_params_, \"model_1\",cv_model_1.best_params_)\n",
    "\n",
    "            predicted_regular_return = best_model_0.predict(X_test)\n",
    "            predicted_default_return = best_model_1.predict(X_test)\n",
    "            \n",
    "            if print_to_screen:\n",
    "                print(\"\")\n",
    "                print(\"Testing r2 scores:\")\n",
    "            # Here we use different testing set to report the performance\n",
    "            test_scores = {'model_0':r2_score(y_test_0,best_model_0.predict(X_test_0)),\n",
    "                              'model_1':r2_score(y_test_1,best_model_1.predict(X_test_1))}\n",
    "            if print_to_screen:\n",
    "                print(\"model_0:\", test_scores['model_0'])\n",
    "                print(\"model_1:\", test_scores['model_1'])\n",
    "\n",
    "            cv_objects = {'model_0':cv_model_0, 'model_1':cv_model_1}\n",
    "            out[ret_col] = { 'model_0':best_model_0, 'model_1':best_model_1, 'predicted_regular_return':predicted_regular_return,\n",
    "                      'predicted_default_return':predicted_default_return,'r2_scores':test_scores }\n",
    "\n",
    "        else:\n",
    "            cv_model = GridSearchCV(model, cv_parameters, scoring='r2')\n",
    "\n",
    "            start_time = time.time()\n",
    "            cv_model.fit(X_train, y_train)\n",
    "            end_time = time.time()\n",
    "\n",
    "            best_model = cv_model.best_estimator_\n",
    "            \n",
    "            if print_to_screen:\n",
    "                if model_name != None:\n",
    "                    print(\"=========================================================\")\n",
    "                    print(\"  Model: \" + model_name + \"  Return column: \" + ret_col)\n",
    "                    print(\"=========================================================\")\n",
    "\n",
    "                print(\"Fit time: \" + str(round(end_time - start_time, 2)) + \" seconds\")\n",
    "                print(\"Optimal parameters:\")\n",
    "                print(cv_model.best_params_)\n",
    "\n",
    "            predicted_return = best_model.predict(X_test)\n",
    "            test_scores = {'model':r2_score(y_test,predicted_return)}\n",
    "            if print_to_screen:\n",
    "                print(\"\")\n",
    "                print(\"Testing r2 score:\", test_scores['model'])\n",
    "\n",
    "            cv_objects = {'model':cv_model}\n",
    "            out[ret_col] = {'model':best_model, 'predicted_return':predicted_return, 'r2_scores':r2_score(y_test,predicted_return)}\n",
    "\n",
    "        # Output the results to a file\n",
    "        if output_to_file:\n",
    "            for i in cv_objects:\n",
    "                # Check whether any of the CV parameters are on the edge of\n",
    "                # the search space\n",
    "                opt_params_on_edge = find_opt_params_on_edge(cv_objects[i])\n",
    "                dump_to_output(model_name + \"::\" + ret_col + \"::search_on_edge\", opt_params_on_edge)\n",
    "                if print_to_screen:\n",
    "                    print(\"Were parameters on edge (\" + i + \") : \" + str(opt_params_on_edge))\n",
    "\n",
    "                # Find out how different the scores are for the different values\n",
    "                # tested for by cross-validation. If they're not too different, then\n",
    "                # even if the parameters are off the edge of the search grid, we should\n",
    "                # be ok\n",
    "                score_variation = find_score_variation(cv_objects[i])\n",
    "                dump_to_output(model_name + \"::\" + ret_col + \"::score_variation\", score_variation)\n",
    "                if print_to_screen:\n",
    "                    print(\"Score variations around CV search grid (\" + i + \") : \" + str(score_variation))\n",
    "\n",
    "                # Print out all the scores\n",
    "                dump_to_output(model_name + \"::all_cv_scores\", str(cv_objects[i].cv_results_['mean_test_score']))\n",
    "                if print_to_screen:\n",
    "                    print(\"All test scores : \" + str(cv_objects[i].cv_results_['mean_test_score']) )\n",
    "\n",
    "                # Dump the AUC to file\n",
    "                dump_to_output( model_name + \"::\" + ret_col + \"::r2\", test_scores[i] )\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dic_reg=prepare_data_reg(n_samples_train=644022, n_samples_test=276032, feature_subset=your_features_without_derived)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $l_1$ regularized linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "  Model: l1 reg - together  Return column: ret_PESS\n",
      "=========================================================\n",
      "Fit time: 3.13 seconds\n",
      "Optimal parameters:\n",
      "{'alpha': 0.01}\n",
      "\n",
      "Testing r2 score: -5.469220329734625e-06\n",
      "=========================================================\n",
      "  Model: l1 reg - together  Return column: ret_OPT\n",
      "=========================================================\n",
      "Fit time: 2.71 seconds\n",
      "Optimal parameters:\n",
      "{'alpha': 0.001}\n",
      "\n",
      "Testing r2 score: 0.0030948755197958144\n",
      "=========================================================\n",
      "  Model: l1 reg - together  Return column: ret_INTa\n",
      "=========================================================\n",
      "Fit time: 2.71 seconds\n",
      "Optimal parameters:\n",
      "{'alpha': 0.01}\n",
      "\n",
      "Testing r2 score: -8.423298272219881e-06\n",
      "=========================================================\n",
      "  Model: l1 reg - together  Return column: ret_INTb\n",
      "=========================================================\n",
      "Fit time: 2.86 seconds\n",
      "Optimal parameters:\n",
      "{'alpha': 0.01}\n",
      "\n",
      "Testing r2 score: -9.21856901281437e-06\n"
     ]
    }
   ],
   "source": [
    "## First, trying l1 regularized linear regression with hyper-parameters\n",
    "\n",
    "l1_lin_reg =  linear_model.Lasso()\n",
    "\n",
    "cv_parameters = {'alpha':[0.001, 0.01, 0.1, 0.5, 0.75, 1]}\n",
    "\n",
    "#Training both our models together\n",
    "reg_lasso = fit_regression(l1_lin_reg, data_dic_reg,\n",
    "                      cv_parameters = cv_parameters,\n",
    "                      separate = False, \n",
    "                      model_name = 'l1 reg - together',\n",
    "                      random_state = default_seed,\n",
    "                      output_to_file = False,\n",
    "                      print_to_screen = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "  Model: l1 reg - Seperate  Return column: ret_PESS\n",
      "=========================================================\n",
      "Fit time: 1.94 seconds\n",
      "Optimal parameters:\n",
      "model_0: {'alpha': 0.001} model_1 {'alpha': 0.001}\n",
      "\n",
      "Testing r2 scores:\n",
      "model_0: 0.1501638097870679\n",
      "model_1: 0.015309636509529434\n",
      "=========================================================\n",
      "  Model: l1 reg - Seperate  Return column: ret_OPT\n",
      "=========================================================\n",
      "Fit time: 2.14 seconds\n",
      "Optimal parameters:\n",
      "model_0: {'alpha': 0.001} model_1 {'alpha': 0.001}\n",
      "\n",
      "Testing r2 scores:\n",
      "model_0: 0.6789601298875565\n",
      "model_1: 0.015189136378520929\n",
      "=========================================================\n",
      "  Model: l1 reg - Seperate  Return column: ret_INTa\n",
      "=========================================================\n",
      "Fit time: 2.11 seconds\n",
      "Optimal parameters:\n",
      "model_0: {'alpha': 0.001} model_1 {'alpha': 0.01}\n",
      "\n",
      "Testing r2 scores:\n",
      "model_0: 0.2602540149867293\n",
      "model_1: -5.765363061538409e-05\n",
      "=========================================================\n",
      "  Model: l1 reg - Seperate  Return column: ret_INTb\n",
      "=========================================================\n",
      "Fit time: 2.37 seconds\n",
      "Optimal parameters:\n",
      "model_0: {'alpha': 0.001} model_1 {'alpha': 0.01}\n",
      "\n",
      "Testing r2 scores:\n",
      "model_0: 0.38636531329713397\n",
      "model_1: -5.912520113304787e-05\n"
     ]
    }
   ],
   "source": [
    "## First, trying l1 regularized linear regression with hyper-parameters - two separate regressors\n",
    "\n",
    "l1_lin_reg =  linear_model.Lasso()\n",
    "\n",
    "cv_parameters = {'alpha':[0.001, 0.01, 0.1, 0.5, 0.75, 1]}\n",
    "\n",
    "#Training both our models separately\n",
    "reg_lasso = fit_regression(l1_lin_reg, data_dic_reg,\n",
    "                      cv_parameters = cv_parameters,\n",
    "                      separate = True, \n",
    "                      model_name = 'l1 reg - Seperate',\n",
    "                      random_state = default_seed,\n",
    "                      output_to_file = False,\n",
    "                      print_to_screen = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $l_2$ regularized linear regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "  Model: l2 reg - together  Return column: ret_PESS\n",
      "=========================================================\n",
      "Fit time: 3.0 seconds\n",
      "Optimal parameters:\n",
      "{'alpha': 0.001}\n",
      "\n",
      "Testing r2 score: 0.01953613907990248\n",
      "=========================================================\n",
      "  Model: l2 reg - together  Return column: ret_OPT\n",
      "=========================================================\n",
      "Fit time: 2.49 seconds\n",
      "Optimal parameters:\n",
      "{'alpha': 0.01}\n",
      "\n",
      "Testing r2 score: 0.014491489763472032\n",
      "=========================================================\n",
      "  Model: l2 reg - together  Return column: ret_INTa\n",
      "=========================================================\n",
      "Fit time: 2.74 seconds\n",
      "Optimal parameters:\n",
      "{'alpha': 0.001}\n",
      "\n",
      "Testing r2 score: 0.020297571402463577\n",
      "=========================================================\n",
      "  Model: l2 reg - together  Return column: ret_INTb\n",
      "=========================================================\n",
      "Fit time: 2.54 seconds\n",
      "Optimal parameters:\n",
      "{'alpha': 0.001}\n",
      "\n",
      "Testing r2 score: 0.019841995975299676\n"
     ]
    }
   ],
   "source": [
    "## trying l2 regularized linear regression with hyper-parameters\n",
    "\n",
    "l2_lin_reg =  linear_model.Ridge()\n",
    "\n",
    "cv_parameters = {'alpha':[0.001, 0.01, 0.1, 0.5, 0.75, 1]}\n",
    "\n",
    "#Training both our models together\n",
    "reg_lasso = fit_regression(l2_lin_reg, data_dic_reg,\n",
    "                      cv_parameters = cv_parameters,\n",
    "                      separate = False, \n",
    "                      model_name = 'l2 reg - together',\n",
    "                      random_state = default_seed,\n",
    "                      output_to_file = False,\n",
    "                      print_to_screen = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "  Model: l2 reg - Separate  Return column: ret_PESS\n",
      "=========================================================\n",
      "Fit time: 1.51 seconds\n",
      "Optimal parameters:\n",
      "model_0: {'alpha': 1} model_1 {'alpha': 0.001}\n",
      "\n",
      "Testing r2 scores:\n",
      "model_0: 0.2600604175988761\n",
      "model_1: 0.09875940490402768\n",
      "=========================================================\n",
      "  Model: l2 reg - Separate  Return column: ret_OPT\n",
      "=========================================================\n",
      "Fit time: 1.75 seconds\n",
      "Optimal parameters:\n",
      "model_0: {'alpha': 1} model_1 {'alpha': 0.001}\n",
      "\n",
      "Testing r2 scores:\n",
      "model_0: 0.7131126222275032\n",
      "model_1: 0.09388250202188342\n",
      "=========================================================\n",
      "  Model: l2 reg - Separate  Return column: ret_INTa\n",
      "=========================================================\n",
      "Fit time: 1.81 seconds\n",
      "Optimal parameters:\n",
      "model_0: {'alpha': 1} model_1 {'alpha': 0.001}\n",
      "\n",
      "Testing r2 scores:\n",
      "model_0: 0.38214479669503076\n",
      "model_1: 0.01872446069291589\n",
      "=========================================================\n",
      "  Model: l2 reg - Separate  Return column: ret_INTb\n",
      "=========================================================\n",
      "Fit time: 1.7 seconds\n",
      "Optimal parameters:\n",
      "model_0: {'alpha': 1} model_1 {'alpha': 0.001}\n",
      "\n",
      "Testing r2 scores:\n",
      "model_0: 0.5069980628571837\n",
      "model_1: 0.019975850480919988\n"
     ]
    }
   ],
   "source": [
    "## trying l2 regularized linear regression with hyper-parameters - two separate regressors\n",
    "\n",
    "l2_lin_reg =  linear_model.Ridge()\n",
    "\n",
    "cv_parameters = {'alpha':[0.001, 0.01, 0.1, 0.5, 0.75, 1]}\n",
    "\n",
    "#Training both our models separately\n",
    "reg_lasso = fit_regression(l2_lin_reg, data_dic_reg,\n",
    "                      cv_parameters = cv_parameters,\n",
    "                      separate = True, \n",
    "                      model_name = 'l2 reg - Separate',\n",
    "                      random_state = default_seed,\n",
    "                      output_to_file = False,\n",
    "                      print_to_screen = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer perceptron regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on documentation, we are only choosing 'adam' (default) since we are dealing with a large dataset\n",
    "#we have changed it here due to the amount of time it takes to run\n",
    "param_grid = { \n",
    "    'hidden_layer_sizes': [(10,10), (20,20), (30, 40)],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'activation': ['logistic', 'tanh', 'relu'],\n",
    "    'learning_rate': ['constant','adaptive','invscaling']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "  Model: MLP reg - together  Return column: ret_PESS\n",
      "=========================================================\n",
      "Fit time: 3384.7 seconds\n",
      "Optimal parameters:\n",
      "{'activation': 'logistic', 'alpha': 0.0001, 'hidden_layer_sizes': (30, 40), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "\n",
      "Testing r2 score: 0.017230126939047463\n",
      "=========================================================\n",
      "  Model: MLP reg - together  Return column: ret_OPT\n",
      "=========================================================\n",
      "Fit time: 3336.85 seconds\n",
      "Optimal parameters:\n",
      "{'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (30, 40), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "\n",
      "Testing r2 score: 0.024770965584946736\n",
      "=========================================================\n",
      "  Model: MLP reg - together  Return column: ret_INTa\n",
      "=========================================================\n",
      "Fit time: 3217.39 seconds\n",
      "Optimal parameters:\n",
      "{'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (30, 40), 'learning_rate': 'invscaling', 'solver': 'adam'}\n",
      "\n",
      "Testing r2 score: 0.024743376035918918\n",
      "=========================================================\n",
      "  Model: MLP reg - together  Return column: ret_INTb\n",
      "=========================================================\n",
      "Fit time: 3225.63 seconds\n",
      "Optimal parameters:\n",
      "{'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (20, 20), 'learning_rate': 'invscaling', 'solver': 'adam'}\n",
      "\n",
      "Testing r2 score: 0.030530635654156812\n"
     ]
    }
   ],
   "source": [
    "## trying multi-layer perceptron regression with hyper-parameters\n",
    "\n",
    "mlp_regressor = MLPRegressor()\n",
    "\n",
    "#Training both our models together\n",
    "reg_mlp = fit_regression(mlp_regressor, data_dic_reg,\n",
    "                      cv_parameters = param_grid,\n",
    "                      separate = False, \n",
    "                      model_name = 'MLP reg - together',\n",
    "                      random_state = default_seed,\n",
    "                      output_to_file = False,\n",
    "                      print_to_screen = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "  Model: MLP reg - separate  Return column: ret_PESS\n",
      "=========================================================\n",
      "Fit time: 2972.47 seconds\n",
      "Optimal parameters:\n",
      "model_0: {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (20, 20), 'learning_rate': 'invscaling', 'solver': 'adam'} model_1 {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (20, 20), 'learning_rate': 'invscaling', 'solver': 'adam'}\n",
      "\n",
      "Testing r2 scores:\n",
      "model_0: 0.2863975713933228\n",
      "model_1: 0.12205099289460708\n",
      "=========================================================\n",
      "  Model: MLP reg - separate  Return column: ret_OPT\n",
      "=========================================================\n",
      "Fit time: 2724.45 seconds\n",
      "Optimal parameters:\n",
      "model_0: {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (10, 10), 'learning_rate': 'adaptive', 'solver': 'adam'} model_1 {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (20, 20), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "\n",
      "Testing r2 scores:\n",
      "model_0: 0.7224689956963188\n",
      "model_1: 0.07657550074592157\n",
      "=========================================================\n",
      "  Model: MLP reg - separate  Return column: ret_INTa\n",
      "=========================================================\n",
      "Fit time: 2699.6 seconds\n",
      "Optimal parameters:\n",
      "model_0: {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (20, 20), 'learning_rate': 'constant', 'solver': 'adam'} model_1 {'activation': 'logistic', 'alpha': 0.0001, 'hidden_layer_sizes': (30, 40), 'learning_rate': 'invscaling', 'solver': 'adam'}\n",
      "\n",
      "Testing r2 scores:\n",
      "model_0: 0.42197779275129965\n",
      "model_1: 0.011734304936058315\n",
      "=========================================================\n",
      "  Model: MLP reg - separate  Return column: ret_INTb\n",
      "=========================================================\n",
      "Fit time: 2699.7 seconds\n",
      "Optimal parameters:\n",
      "model_0: {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (30, 40), 'learning_rate': 'adaptive', 'solver': 'adam'} model_1 {'activation': 'logistic', 'alpha': 0.05, 'hidden_layer_sizes': (30, 40), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "\n",
      "Testing r2 scores:\n",
      "model_0: 0.49227000002175136\n",
      "model_1: -0.007160609675561869\n"
     ]
    }
   ],
   "source": [
    "## trying multi-layer perceptron regression with hyper-parameters separate regressors\n",
    "\n",
    "mlp_regressor = MLPRegressor()\n",
    "\n",
    "#Training both our models separately\n",
    "reg_mlp = fit_regression(mlp_regressor, data_dic_reg,\n",
    "                      cv_parameters = param_grid,\n",
    "                      separate = True, \n",
    "                      model_name = 'MLP reg - separate',\n",
    "                      random_state = default_seed,\n",
    "                      output_to_file = False,\n",
    "                      print_to_screen = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "  Model: Random Forest - separator  Return column: ret_PESS\n",
      "=========================================================\n",
      "Fit time: 1503.3 seconds\n",
      "Optimal parameters:\n",
      "{'max_depth': 8, 'n_estimators': 500}\n",
      "\n",
      "Testing r2 score: 0.03366189016254317\n",
      "=========================================================\n",
      "  Model: Random Forest - separator  Return column: ret_OPT\n",
      "=========================================================\n",
      "Fit time: 1547.16 seconds\n",
      "Optimal parameters:\n",
      "{'max_depth': 8, 'n_estimators': 500}\n",
      "\n",
      "Testing r2 score: 0.023981266185391648\n",
      "=========================================================\n",
      "  Model: Random Forest - separator  Return column: ret_INTa\n",
      "=========================================================\n",
      "Fit time: 1602.27 seconds\n",
      "Optimal parameters:\n",
      "{'max_depth': 8, 'n_estimators': 500}\n",
      "\n",
      "Testing r2 score: 0.03620367695649951\n",
      "=========================================================\n",
      "  Model: Random Forest - separator  Return column: ret_INTb\n",
      "=========================================================\n",
      "Fit time: 1617.27 seconds\n",
      "Optimal parameters:\n",
      "{'max_depth': 8, 'n_estimators': 500}\n",
      "\n",
      "Testing r2 score: 0.034761862659234866\n"
     ]
    }
   ],
   "source": [
    "## trying random forest regression with hyper-parameters\n",
    "#limited range of hyper parameters since it takes a lot of time.\n",
    "\n",
    "param_grid = { \n",
    "    'n_estimators': [200, 500],\n",
    "    'max_depth' : [4,6,8]\n",
    "}\n",
    "\n",
    "#adding n_jobs = -1\n",
    "rf_regressor = RandomForestRegressor(n_jobs = -1)\n",
    "\n",
    "#Training both our models together\n",
    "reg_rf = fit_regression(rf_regressor, data_dic_reg,\n",
    "                      cv_parameters = param_grid,\n",
    "                      separate = False, \n",
    "                      model_name = 'Random Forest - separator',\n",
    "                      random_state = default_seed,\n",
    "                      output_to_file = False,\n",
    "                      print_to_screen = True)\n",
    "\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "#Note: The name of the model is actually supposed to be \"together\",\n",
    "# since the separate parameter is set to False. We mistakenly \n",
    "# wrote \"separate\" here.\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "  Model: Random Forest - separator  Return column: ret_PESS\n",
      "=========================================================\n",
      "Fit time: 1440.39 seconds\n",
      "Optimal parameters:\n",
      "model_0: {'max_depth': 8, 'n_estimators': 200} model_1 {'max_depth': 8, 'n_estimators': 500}\n",
      "\n",
      "Testing r2 scores:\n",
      "model_0: 0.3270334640215452\n",
      "model_1: 0.12570386368488795\n",
      "=========================================================\n",
      "  Model: Random Forest - separator  Return column: ret_OPT\n",
      "=========================================================\n",
      "Fit time: 1451.08 seconds\n",
      "Optimal parameters:\n",
      "model_0: {'max_depth': 8, 'n_estimators': 500} model_1 {'max_depth': 8, 'n_estimators': 200}\n",
      "\n",
      "Testing r2 scores:\n",
      "model_0: 0.7325639777463089\n",
      "model_1: 0.11719482225985811\n",
      "=========================================================\n",
      "  Model: Random Forest - separator  Return column: ret_INTa\n",
      "=========================================================\n",
      "Fit time: 1500.01 seconds\n",
      "Optimal parameters:\n",
      "model_0: {'max_depth': 8, 'n_estimators': 500} model_1 {'max_depth': 8, 'n_estimators': 500}\n",
      "\n",
      "Testing r2 scores:\n",
      "model_0: 0.44703283398459603\n",
      "model_1: 0.04790769315400645\n",
      "=========================================================\n",
      "  Model: Random Forest - separator  Return column: ret_INTb\n",
      "=========================================================\n",
      "Fit time: 1487.09 seconds\n",
      "Optimal parameters:\n",
      "model_0: {'max_depth': 8, 'n_estimators': 500} model_1 {'max_depth': 8, 'n_estimators': 200}\n",
      "\n",
      "Testing r2 scores:\n",
      "model_0: 0.5581649361158212\n",
      "model_1: 0.049168678340471894\n"
     ]
    }
   ],
   "source": [
    "## trying random forest regression with hyper-parameters\n",
    "# limited range of hyperparameters\n",
    "param_grid = { \n",
    "    'n_estimators': [200, 500],\n",
    "    'max_depth' : [4,6,8]\n",
    "}\n",
    "\n",
    "rf_regressor = RandomForestRegressor(n_jobs=-1)\n",
    "\n",
    "#Training both our models separately\n",
    "reg_rf = fit_regression(rf_regressor, data_dic_reg,\n",
    "                      cv_parameters = param_grid,\n",
    "                      separate = True, \n",
    "                      model_name = 'Random Forest - separator',\n",
    "                      random_state = default_seed,\n",
    "                      output_to_file = False,\n",
    "                      print_to_screen = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test investment strategies\n",
    "\n",
    "Now we test several investment strategies using the learning models above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_investments(data_dict,\n",
    "                        classifier = None,\n",
    "                        regressor = None,\n",
    "                        strategy = 'Random', \n",
    "                        num_loans = 1000,\n",
    "                        random_state = default_seed,\n",
    "                        output_to_file = True):\n",
    "    '''\n",
    "    This function tests a variety of investment methodologies and their returns. \n",
    "    It will run its tests on the loans defined by the test_set element of the data\n",
    "    dictionary.\n",
    "    \n",
    "    It is currently able to test four strategies\n",
    "      - random: invest in a random set of loans\n",
    "      - default-based: score each loan by probability of default, and only invest\n",
    "                 in the \"safest\" loans (i.e., those with the lowest probabilities\n",
    "                 of default)\n",
    "      - return-based: train a single regression model to predict the expected return\n",
    "                    of loans in the past. Then, for loans we could invest in, simply\n",
    "                    rank them by their expected returns and invest in that order.\n",
    "      - default-& return-based: train two regression models to predict the expected return of\n",
    "                   defaulted loans and non-defaulted loans in the training set. Then,\n",
    "                   for each potential loan we could invest in, predict the probability\n",
    "                   the loan will default, its return if it doesn't default and its\n",
    "                   return if it does. Then, calculate a weighted combination of\n",
    "                   the latter using the former to find a predicted return. Rank the\n",
    "                   loans by this expected return, and invest in that order\n",
    "    \n",
    "    It expects the following parameters\n",
    "      - data_dict: the dictionary containing both training and testing data;\n",
    "                   returned by the prepare_data function\n",
    "      - classifier: a fitted model object which is returned by the fit_classification function.\n",
    "      - regressor: a fitted model object which is returned by the fit_regression function.\n",
    "      - strategy: the name of the strategy; one of the three listed above\n",
    "      - num_loans: the number of loans to be included in the test portfolio\n",
    "      - num_samples: the number of random samples used to compute average return ()   \n",
    "      - random_state: the random seed to use when selecting a subset of rows\n",
    "      - output_to_file: if the results will be saved to the output file\n",
    "      \n",
    "    The function returns a dictionary FOR EACH RETURN DEFINITION with the following entries\n",
    "      - strategy: the name of the strategy\n",
    "      - average return: the return of the strategy based on the testing set\n",
    "      - test data: the updated Dataframe of testing data. Useful in the optimization section\n",
    "    '''\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Retrieve the rows that were used to train and test  the\n",
    "    # classification model\n",
    "    train_set = data_dict['train_set']\n",
    "    test_set = data_dict['test_set']\n",
    "    \n",
    "    col_list = ['ret_PESS', 'ret_OPT', 'ret_INTa', 'ret_INTb']\n",
    "    \n",
    "    # Create a dataframe for testing, including the score\n",
    "    data_test = data.loc[test_set,:]\n",
    "    out = {}\n",
    "    \n",
    "    for ret_col in col_list:    \n",
    "    \n",
    "        if strategy == 'Random':\n",
    "            # Randomize the order of the rows in the datframe\n",
    "            data_test = data_test.sample(frac = 1).reset_index(drop = True)\n",
    "\n",
    "            ## Select num_loans to invest in\n",
    "            pf_test = data_test[['funded_amnt',ret_col]].iloc[:num_loans]\n",
    "\n",
    "            ## Find the average return for these loans\n",
    "            ret_test = np.dot(pf_test[ret_col],pf_test.funded_amnt)/np.sum(pf_test.funded_amnt)\n",
    "\n",
    "            # Return\n",
    "            out[ret_col] = {'strategy':strategy, 'average return':ret_test}\n",
    "\n",
    "            # Dump the strategy performance to file\n",
    "            if output_to_file:\n",
    "                dump_to_output(strategy + \",\" + ret_col + \"::average return\", ret_test )\n",
    "\n",
    "            continue\n",
    "        \n",
    "        elif strategy == 'Return-based':\n",
    "            \n",
    "            colname = 'predicted_return_' + ret_col \n",
    "\n",
    "            data_test[colname] = regressor[ret_col]['predicted_return']\n",
    "\n",
    "            # Sort the loans by predicted return\n",
    "            data_test = data_test.sort_values(by=colname, ascending = False).reset_index(drop = True)\n",
    "\n",
    "            ## Pick num_loans loans\n",
    "            pf_test = data_test[['funded_amnt',ret_col]].iloc[:num_loans]\n",
    "\n",
    "            ## Find their return\n",
    "            ret_test = np.dot(pf_test[ret_col],pf_test.funded_amnt)/np.sum(pf_test.funded_amnt)\n",
    "\n",
    "            # Return\n",
    "            out[ret_col] = {'strategy':strategy, 'average return':ret_test, 'test data':data_test}\n",
    "\n",
    "            # Dump the strategy performance to file\n",
    "            if output_to_file:\n",
    "                dump_to_output(strategy + \",\" + ret_col + \"::average return\", ret_test )\n",
    "\n",
    "            continue\n",
    "            \n",
    "        # Get the predicted scores, if the strategy is not Random or just Regression\n",
    "        try:\n",
    "            y_pred_score = classifier['y_pred_probs']\n",
    "        except:\n",
    "            y_pred_score = classifier['y_pred_score']\n",
    "\n",
    "        data_test['score'] = y_pred_score\n",
    "\n",
    "\n",
    "        if strategy == 'Default-based':\n",
    "            # Sort the test data by the score\n",
    "            data_test = data_test.sort_values(by='score').reset_index(drop = True)\n",
    "\n",
    "            ## Select num_loans to invest in\n",
    "            pf_test = data_test[['funded_amnt',ret_col]].iloc[:num_loans]\n",
    "\n",
    "            ## Find the average return for these loans\n",
    "            ret_test =  np.dot(pf_test[ret_col],pf_test.funded_amnt)/np.sum(pf_test.funded_amnt)\n",
    "\n",
    "            # Return\n",
    "            out[ret_col] = {'strategy':strategy, 'average return':ret_test}\n",
    "\n",
    "            # Dump the strategy performance to file\n",
    "            if output_to_file:\n",
    "                dump_to_output(strategy + \",\" + ret_col + \"::average return\", ret_test )\n",
    "\n",
    "            continue\n",
    "\n",
    "\n",
    "        elif strategy == 'Default-return-based':\n",
    "\n",
    "            # Load the predicted returns\n",
    "            data_test['predicted_regular_return'] = regressor[ret_col]['predicted_regular_return']\n",
    "            data_test['predicted_default_return'] = regressor[ret_col]['predicted_default_return']\n",
    "\n",
    "            # Compute expectation\n",
    "            colname = 'predicted_return_' + ret_col \n",
    "            \n",
    "            data_test[colname] = ( (1-data_test.score)*data_test.predicted_regular_return + \n",
    "                                             data_test.score*data_test.predicted_default_return )\n",
    "\n",
    "            # Sort the loans by predicted return\n",
    "            data_test = data_test.sort_values(by=colname, ascending = False).reset_index(drop = True)\n",
    "\n",
    "            ## Pick num_loans loans\n",
    "            pf_test = data_test[['funded_amnt',ret_col]].iloc[:num_loans]\n",
    "\n",
    "            ## Find their return\n",
    "            ret_test = np.dot(pf_test[ret_col],pf_test.funded_amnt)/np.sum(pf_test.funded_amnt)\n",
    "\n",
    "            # Return\n",
    "            out[ret_col] = {'strategy':strategy, 'average return':ret_test, 'test data':data_test}\n",
    "\n",
    "            # Dump the strategy performance to file\n",
    "            if output_to_file:\n",
    "                dump_to_output(strategy + \",\" + ret_col + \"::average return\", ret_test )\n",
    "\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            return 'Not a valid strategy'\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create 100 train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do train/test split based on what TAs say, we think we just need to create 100 new samples, and then run each of the \n",
    "#potential investment strategies for each of the sets\n",
    "# prepare the train, test data for training models\n",
    "\n",
    "data_dictionaries_reg = []\n",
    "for i in range(100):\n",
    "    data_reg = prepare_data(feature_subset = your_features_without_derived, random_state=i, n_samples_train=30000, n_samples_test=20000)\n",
    "    data_dictionaries_reg.append(data_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strategy: Random\n",
      "ret_PESS Mean: 0.006254038498643195 Standard Deviation: 0.002995084339371043\n",
      "ret_OPT Mean: 0.053791270927900106 Standard Deviation: 0.0043785031735328475\n",
      "ret_INTa Mean: 0.02247220760429821 Standard Deviation: 0.002858144148899736\n",
      "ret_INTb Mean: 0.05859597754464705 Standard Deviation: 0.0026973748012234715\n"
     ]
    }
   ],
   "source": [
    "## Test investment strategies using the random strategy\n",
    "\n",
    "col_list = ['ret_PESS', 'ret_OPT', 'ret_INTa', 'ret_INTb']\n",
    "test_strategy = 'Random'\n",
    "strat_rand_scores = {'ret_PESS': [], 'ret_OPT': [], 'ret_INTa': [], 'ret_INTb': []}\n",
    "\n",
    "print('strategy:',test_strategy)  \n",
    "for d in data_dictionaries_reg:\n",
    "    strat_rand = test_investments(d,strategy = test_strategy, \n",
    "                                  num_loans = 1000, output_to_file = False)\n",
    "    for ret_col in col_list:\n",
    "        strat_rand_scores[ret_col].append(strat_rand[ret_col]['average return'])\n",
    "\n",
    "for ret_col in col_list:\n",
    "    print(ret_col + ' Mean: ' + str(np.array(strat_rand_scores[ret_col]).mean()) + \n",
    "          ' Standard Deviation: ' + str(np.array(strat_rand_scores[ret_col]).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strategy: Default-based\n",
      "ret_PESS Mean: 0.018513277929074094 Standard Deviation: 0.0013989584976524519\n",
      "ret_OPT Mean: 0.05138821502619492 Standard Deviation: 0.004387413032866921\n",
      "ret_INTa Mean: 0.0234689796992378 Standard Deviation: 0.0027004450256796217\n",
      "ret_INTb Mean: 0.05877613833530702 Standard Deviation: 0.0028511048595744825\n"
     ]
    }
   ],
   "source": [
    "test_strategy = 'Default-based'\n",
    "col_list = ['ret_PESS', 'ret_OPT', 'ret_INTa', 'ret_INTb']\n",
    "strat_def_scores = {'ret_PESS': [], 'ret_OPT': [], 'ret_INTa': [], 'ret_INTb': []}\n",
    "\n",
    "print('strategy:',test_strategy)\n",
    "params = {\n",
    "    'criterion': ['gini'], 'max_depth': [8], 'max_features': ['log2'], 'n_estimators':[500]\n",
    "}\n",
    "##################################################################\n",
    "# We used the hyperparameter values from YOURMODEL.\n",
    "##################################################################\n",
    "for d in data_dictionaries_reg:\n",
    "    classifier_dataset, a, b = fit_classification(RandomForestClassifier(n_jobs=-1), d,\n",
    "                          cv_parameters = params,\n",
    "                          model_name = 'RF Best Model',\n",
    "                          random_state = default_seed,\n",
    "                          output_to_file = False,\n",
    "                          print_to_screen = False)\n",
    "    strat_def = test_investments(d, classifier=classifier_dataset, strategy = test_strategy, \n",
    "                                  num_loans = 1000, output_to_file = False)\n",
    "    for ret_col in col_list:\n",
    "        strat_def_scores[ret_col].append(strat_def[ret_col]['average return'])\n",
    "\n",
    "for ret_col in col_list:\n",
    "    print(ret_col + ' Mean: ' + str(np.array(strat_def_scores[ret_col]).mean()) + \n",
    "          ' Standard Deviation: ' + str(np.array(strat_def_scores[ret_col]).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strategy: Return-based\n",
      "ret_PESS Mean: 0.017975369606190436 Standard Deviation: 0.0036163445060344245\n",
      "ret_OPT Mean: 0.05226360926032089 Standard Deviation: 0.004230362246472205\n",
      "ret_INTa Mean: 0.02366447666466596 Standard Deviation: 0.002820150283831626\n",
      "ret_INTb Mean: 0.05875312672259085 Standard Deviation: 0.0031857820826836076\n"
     ]
    }
   ],
   "source": [
    "test_strategy = 'Return-based'\n",
    "\n",
    "strat_ret_scores = {'ret_PESS': [], 'ret_OPT': [], 'ret_INTa': [], 'ret_INTb': []}\n",
    "col_list = ['ret_PESS', 'ret_OPT', 'ret_INTa', 'ret_INTb']\n",
    "print('strategy:',test_strategy)  \n",
    "##################################################################\n",
    "#We are using default parameters, but we chose the best model from the regression models above as it mentioned\n",
    "# \"simple\" regression not \"best regression\"\n",
    "##################################################################\n",
    "for d in data_dictionaries_reg:\n",
    "    reg_rf_separate = fit_regression(RandomForestRegressor(n_jobs=-1), d,\n",
    "                       cv_parameters = {}, separate = False, model_name = \"RF Best Model all returns\", random_state=default_seed, \n",
    "                       output_to_file = False, print_to_screen=False)\n",
    "    strat_ret = test_investments(d, regressor=reg_rf_separate, strategy = test_strategy, \n",
    "                                  num_loans = 1000, output_to_file = False)\n",
    "    for ret_col in col_list:\n",
    "        strat_ret_scores[ret_col].append(strat_ret[ret_col]['average return'])\n",
    "\n",
    "for ret_col in col_list:\n",
    "    print(ret_col + ' Mean: ' + str(np.array(strat_ret_scores[ret_col]).mean()) + \n",
    "          ' Standard Deviation: ' + str(np.array(strat_ret_scores[ret_col]).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strategy: Default-return-based\n",
      "ret_PESS Mean: 0.017975369606190436 Standard Deviation: 0.0036163445060344245\n",
      "ret_OPT Mean: 0.05226360926032089 Standard Deviation: 0.004230362246472205\n",
      "ret_INTa Mean: 0.02366447666466596 Standard Deviation: 0.002820150283831626\n",
      "ret_INTb Mean: 0.05875312672259085 Standard Deviation: 0.0031857820826836076\n"
     ]
    }
   ],
   "source": [
    "test_strategy = 'Default-return-based'\n",
    "col_list = ['ret_PESS', 'ret_OPT', 'ret_INTa', 'ret_INTb']\n",
    "strat_default_ret_scores = {'ret_PESS': [], 'ret_OPT': [], 'ret_INTa': [], 'ret_INTb': []}\n",
    "params = {\n",
    "    'criterion': ['gini'], 'max_depth': [8], 'max_features': ['log2'], 'n_estimators':[500]\n",
    "}\n",
    "## For the Default-return-based strategy we need to fit a new regressor with separate = True\n",
    "\n",
    "\n",
    "print('strategy:',test_strategy)\n",
    "##################################################################\n",
    "# We used YOURMODEL's best parameters and a simple RandomForestRegressor() model\n",
    "##################################################################\n",
    "for d in data_dictionaries_reg:\n",
    "    reg_rf_separate = fit_regression(RandomForestRegressor(n_jobs=-1), d,\n",
    "                       cv_parameters = {}, separate = True, model_name = \"RF separated\", random_state=default_seed, \n",
    "                       output_to_file = False, print_to_screen=False)\n",
    "    classifier_dataset, a, b = fit_classification(RandomForestClassifier(n_jobs=-1), d,\n",
    "                          cv_parameters = params,\n",
    "                          model_name = 'RF Best Model',\n",
    "                          random_state = default_seed,\n",
    "                          output_to_file = False,\n",
    "                          print_to_screen = False)\n",
    "    strat_default_ret = test_investments(d, classifier=classifier_dataset, regressor=reg_rf_separate, \n",
    "                                         strategy = test_strategy, num_loans = 1000, output_to_file = False)\n",
    "    for ret_col in col_list:\n",
    "        strat_default_ret_scores[ret_col].append(strat_default_ret[ret_col]['average return'])\n",
    "\n",
    "for ret_col in col_list:\n",
    "    print(ret_col + ' Mean: ' + str(np.array(strat_ret_scores[ret_col]).mean()) + \n",
    "          ' Standard Deviation: ' + str(np.array(strat_ret_scores[ret_col]).std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEST Solution for top 1000 Loans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best M1: 0.14362241273214826 best M2: 0.409636971347599 best M3: 0.16286272967985216 best M4: 0.20355522442151647\n"
     ]
    }
   ],
   "source": [
    "#Best Loans in Hindsight - choosing the loans with the highest returns for each type of return\n",
    "#Get all the return values\n",
    "ret1=list(data.ret_PESS)\n",
    "ret2=list(data.ret_OPT)\n",
    "ret3=list(data.ret_INTa)\n",
    "ret4=list(data.ret_INTb)\n",
    "\n",
    "#Sort in descending order\n",
    "ret1.sort(reverse = True)\n",
    "ret2.sort(reverse = True)\n",
    "ret3.sort(reverse = True)\n",
    "ret4.sort(reverse = True)\n",
    "\n",
    "best1=sum(ret1[:1000])/1000\n",
    "best2=sum(ret2[:1000])/1000\n",
    "best3=sum(ret3[:1000])/1000\n",
    "best4=sum(ret4[:1000])/1000\n",
    "\n",
    "print('best M1:',best1,'best M2:',best2,'best M3:',best3,'best M4:',best4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity test of portfolio size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Test the best-performing data-driven strategy on different portfolio sizes\n",
    "##Please note we didn't run 100 rounds this time becasue the default-based strategy takes about 30 mins per dataset,\n",
    "##which would have been very time-consuming\n",
    "result_sensitivity = []\n",
    "average_returns = []\n",
    "params = {\n",
    "    'criterion': ['gini'], 'max_depth': [8], 'max_features': ['log2'], 'n_estimators':[500]\n",
    "}\n",
    "## Vary the portfolio size from 1,000 to 10,000\n",
    "reg_rf_separate = fit_regression(RandomForestRegressor(n_jobs=-1), data_dic_reg,\n",
    "                       cv_parameters = {}, separate = True, model_name = \"RF separated\", random_state=default_seed, \n",
    "                       output_to_file = False, print_to_screen=False)\n",
    "classifier_dataset, a, b = fit_classification(RandomForestClassifier(n_jobs=-1), data_dic_reg,\n",
    "                          cv_parameters = params,\n",
    "                          model_name = 'RF Best Model',\n",
    "                          random_state = default_seed,\n",
    "                          output_to_file = False,\n",
    "                          print_to_screen = False)\n",
    "for num_loans in list(range(1000,10000,1000)):\n",
    "    #### Make sure to do the best strategy from the 4 above\n",
    "    average_returns = []    \n",
    "    reg_0 = test_investments(data_dic_reg, classifier=classifier_dataset, regressor=reg_rf_separate, strategy = 'Default-return-based', \n",
    "                                  num_loans = num_loans, output_to_file = False) \n",
    "    average_returns.append(reg_0['ret_PESS']['average return'])\n",
    "    result_sensitivity.append(np.array(average_returns).mean())\n",
    "    \n",
    "result_sensitivity = np.array(result_sensitivity) * 100\n",
    "sns.pointplot(np.array(list(range(1000,10000,1000))),result_sensitivity)\n",
    "sns.despine()\n",
    "plt.ylabel('Investment Return (%)',size = 14)\n",
    "plt.xlabel('Portfolio Size',size = 14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
